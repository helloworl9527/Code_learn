# Hadoop + Hive + Spark Mac M1 环境搭建 & 项目流程一页总结

> 适用于在 **Mac M‑series + VMware Fusion + CentOS 9 (aarch64)** 环境下快速复现伪分布式大数据开发环境，同时回顾项目从数据落地到 Spark SQL 分析的完整链路。

---

## 1 环境版本一览

| 组件 | 推荐版本 | 备注 |
| --- | --- | --- |
| macOS 主机 | macOS 15 + VMware Fusion | Apple Silicon 原生 |
| Guest OS | CentOS Stream 9 aarch64 | 静态 IP 方便 SSH |
| JDK | 8u362 (或 openjdk‑8) | Hadoop 3.x 兼容最佳 |
| Hadoop | 3.2.0 | 伪分布式即可 |
| Hive | 4.0.1 | 元数据库迁移到 MySQL |
| MySQL | 8.0 | Hive metastore |
| Spark | 3.5.6（或 3.3.3） | 与 Hadoop 3.2.0 二进制兼容 |

---

## 2 目录约定（宿主机 ⇋ 虚拟机）

| 角色 | 典型路径 |
| :--- | :--- |
| Hadoop 安装根 | `/usr/local/hadoop3.2/hadoop-3.2.0` |
| Hadoop 配置 | `$HADOOP_HOME/etc/hadoop` |
| Hadoop 日志 | `/usr/local/hadoop3.2/hadoop_log/data/hadoop_repo/logs/hadoop` |
| Java | `/usr/local/java/jdk8u362-b09` |
| Hive | `/usr/local/Hive/apache-hive-4.0.1-bin` |
| Spark | `/usr/local/Spark/spark-3.5.6-bin-hadoop3` |
| IDEA | `/home/<user>/下载/idea-IU-251.26094.121/bin/idea.sh` |

> **Tips**：统一放在 `/usr/local`，并通过 `~/.bash_profile` 维护 `$HADOOP_HOME`、`$HIVE_HOME`、`$SPARK_HOME` 等环境变量，便于脚本调用。

---

## 3 安装与配置关键步骤

### 3.1 SSH 免密

```bash
# 虚拟机 root
ssh-keygen -t rsa        # 连续回车
ssh-copy-id <host-ip>    # 分发公钥到自身/其它节点
```

- 编辑 `/etc/hosts` 维护 IP ⇋ hostname 映射。  
- 确保目标机器上 `~/.ssh/authorized_keys` 权限为 `600`，目录 `700`.

### 3.2 系统优化

```bash
systemctl disable firewalld --now   # 关闭防火墙
swapoff -a                          # 伪分布可选
```

### 3.3 JDK 安装

```bash
yum install -y java-1.8.0-openjdk-devel   # 或解压官方 JDK 至 /usr/local/java
```

### 3.4 Hadoop 3.2.0 伪分布式

1. 解压 `hadoop-3.2.0.tar.gz` 到 `$HADOOP_HOME`  
2. 修改配置：`core-site.xml`, `hdfs-site.xml`, `mapred-site.xml`, `yarn-site.xml`  
3. **格式化** NameNode：

   ```bash
   hdfs namenode -format
   ```

4. 启动服务：

   ```bash
   $HADOOP_HOME/sbin/start-dfs.sh
   $HADOOP_HOME/sbin/start-yarn.sh
   ```

> 常见错误：Java Home 未配置 / SSH 免密失败 / 文件权限问题。

### 3.5 MySQL 8.0

```bash
dnf install -y mysql-server
systemctl enable --now mysqld
mysql_secure_installation    # 重置 root 密码
```

### 3.6 Hive 4.0.1 + MySQL Metastore

```bash
cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml
# 在 hive-site.xml 中配置：
javax.jdo.option.ConnectionURL=jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8
javax.jdo.option.ConnectionDriverName=com.mysql.cj.jdbc.Driver
javax.jdo.option.ConnectionUserName=root
javax.jdo.option.ConnectionPassword=<your_pass>
hive.metastore.warehouse.dir=/user/hive/warehouse
```

- 拷贝 `mysql-connector-j.jar` 到 `$HIVE_HOME/lib/`  
- 初始化 metastore：

  ```bash
  schematool -initSchema -dbType mysql
  ```

- 启动：

  ```bash
  hive --service metastore &
  beeline -u jdbc:hive2://
  ```

### 3.7 Spark 3.x

```bash
tar -xzvf spark-3.5.6-bin-hadoop3.tgz -C /usr/local/Spark
$SPARK_HOME/sbin/start-all.sh      # 启动 Master/Worker
spark-shell                        # Scala 交互
```

---

## 4 项目典型流程

| 阶段 | 主要操作 |
| --- | --- |
| **数据上载** | `hdfs dfs -mkdir -p /traffic/raw && hdfs dfs -put local/*.txt /traffic/raw` |
| **Hive 建库建表** | `CREATE DATABASE traffic_db;`<br>`CREATE EXTERNAL TABLE monitor_flow_action (...);` |
| **ETL / 统计** | 通过 Hive 或 Spark SQL 运行 `SELECT monitor_id, COUNT(*) …` 等查询 |
| **Spark SQL 分析** | `spark-sql --master local[*] --conf spark.sql.catalogImplementation=hive` |
| **结果输出** | 保存到 HDFS / MySQL / CSV 再下行 |

---

## 5 运维常用命令速查

```bash
# Hadoop
jps                       # 查看 NameNode/DataNode/ResourceManager 状态
hdfs dfsadmin -report     # 查看 HDFS 磁盘使用
yarn application -list    # 当前 YARN 任务

# Hive
beeline -u jdbc:hive2://localhost:10000 -n root

# Spark
$SPARK_HOME/sbin/stop-all.sh
```

---

## 6 常见坑 & 解决方案

| 场景 | 现象 | 处理 |
| --- | --- | --- |
| 启动 Hive 报 `NoSuchObjectException: global_temp` | Spark 默认 catalog 与 Hive 冲突 | 添加 `spark.sql.catalogImplementation=hive` |
| tar 解压报 `unexpected end of file` | 下载包损坏 | 重新下载或 `curl -C - -O <url>` 断点续传 |
| IDEA 无法启动 (`Unable to detect graphics environment`) | CentOS GUI/驱动问题 | 设置 `-Djava.awt.headless=true` 或检查 VMware 3D 加速 |

---

## 7 一言以蔽之

> **核心心法**：先固化 _版本矩阵_，再按“JDK → SSH → Hadoop → MySQL → Hive → Spark”的顺序层层验证；遇错先看 **日志** + **端口** + **权限** 三板斧。

---

_文档基于《hadoop_hive_mac_m1_setup.md》与《项目总结.md》整理，供日后快速回溯。_
