# é¡¹ç›®ä¸ç¯å¢ƒæ­å»ºæ€»ç»“

## ä¸€ã€Mac M1 ä¸Šæ­å»º Hadoop + Hive

# Mac M1 + VMware Fusion å®‰è£…é…ç½® Hadoop + Hive + MySQL å¼€å‘ç¯å¢ƒæŒ‡å—

## ä¸€ã€ç³»ç»Ÿç¯å¢ƒ

- **å®¿ä¸»æœº**ï¼šMacBook Air (M1 èŠ¯ç‰‡)
- **è™šæ‹Ÿæœº**ï¼šCentOS 9 (arrch64æ¶æ„)ï¼Œè¿è¡Œäº VMware Fusion
- **Hadoop**ï¼š3.2.0  
- **Hive**ï¼š4.0.1  
- **MySQL**ï¼š8.0  
- **Java**ï¼šJDK 8u362  
- **Spark**ï¼š3.5.6  

---

## äºŒã€ç›®å½•ç»“æ„çº¦å®š

| ç”¨é€”               | è·¯å¾„ç¤ºä¾‹ |
|--------------------|-----------|
| è½¯ä»¶å‹ç¼©åŒ…å­˜æ”¾ç›®å½• | `/usr/export` |
| è§£å‹åçš„è½¯ä»¶ç›®å½•   | `/usr/local/` |
| Hadoop è·¯å¾„        | `/usr/local/hadoop3.2/hadoop-3.2.0` |
| Hadoop é…ç½®ç›®å½•    | `/usr/local/hadoop3.2/hadoop-3.2.0/etc/hadoop` |
| Hadoop æ—¥å¿—ç›®å½•    | `/usr/local/hadoop3.2/hadoop_log/data/hadoop_repo/logs/hadoop` |
| Java å®‰è£…è·¯å¾„      | `/usr/local/java/jdk8u362-b09` |
| Hive å®‰è£…è·¯å¾„      | `/usr/local/Hive/apache-hive-4.0.1-bin` |
| Spark å®‰è£…è·¯å¾„     | `/usr/local/Spark/spark-3.5.6-bin-hadoop3` |
| IDEA å¯åŠ¨è·¯å¾„      | `/home/cyh/ä¸‹è½½/idea-IU-251.26094.121/bin` å¯åŠ¨ï¼š`./idea.sh` |

---

## ä¸‰ã€è™šæ‹Ÿæœºå®‰è£… CentOS 9

- **ä¸‹è½½åœ°å€**ï¼š  
  - [CentOS å®˜ç½‘](https://www.centos.org/download/)  
  - [é˜¿é‡Œäº‘é•œåƒï¼ˆå»ºè®®ä½¿ç”¨ï¼‰](https://mirrors.aliyun.com/centos-stream/9-stream/BaseOS/aarch64/iso/)  
  - [VMware Fusion å®‰è£…æ•™ç¨‹](https://sysin.org/blog/vmware-fusion-13/)

> ğŸ’¡ Mç³»åˆ—èŠ¯ç‰‡éœ€ä¸‹è½½ arrch64 æ¶æ„çš„ ISO é•œåƒ

---

## å››ã€é…ç½® Hadoop ä¼ªåˆ†å¸ƒå¼é›†ç¾¤

### 1. SSH å…å¯†ç™»å½•é…ç½®

```bash
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh
```

### 2. é‡åˆ°é—®é¢˜è§£å†³æ–¹æ¡ˆ

- **è¿æ¥å¤±è´¥**ï¼š
  - ä½¿ç”¨ `ip addr` æŸ¥å®é™… IPï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ä¸»æœº IPã€‚
- **Permission denied**ï¼š
  - ä¿®æ”¹ `/etc/ssh/sshd_config` æ·»åŠ ï¼š

    ```conf
    PermitRootLogin yes
    PubkeyAuthentication yes
    PasswordAuthentication yes
    ```

  - é‡å¯æœåŠ¡ï¼š`systemctl restart sshd`
- **è„šæœ¬æƒé™ä¸è¶³**ï¼š
  ```bash
  chmod +x /usr/local/hadoop3.2/hadoop-3.2.0/sbin/start-dfs.sh
  ```

---

## äº”ã€Hadoop å¯åŠ¨

```bash
cd /usr/local/hadoop3.2/hadoop-3.2.0
./sbin/start-all.sh
```

---

## å…­ã€å®‰è£…é…ç½® MySQL

### 1. MySQL åˆå§‹é…ç½®

```bash
systemctl start mysqld
systemctl status mysqld
mysql -u root -p
```

### 2. é‡ç½® root å¯†ç 

```bash
sudo systemctl stop mysqld
sudo -u mysql mysqld --skip-grant-tables --skip-networking &
mysql -u root
FLUSH PRIVILEGES;
ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass123!';
```

---

## ä¸ƒã€å®‰è£…é…ç½® Hive

### 1. é…ç½® Hive ä½¿ç”¨ MySQL ä½œä¸ºå…ƒæ•°æ®åº“

#### ä¿®æ”¹ `hive-site.xml`ï¼š

```xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive_meta?createDatabaseIfNotExist=true&amp;serverTimezone=UTC&amp;useSSL=false</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.cj.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hiveuser</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>Hive123@!</value>
</property>
```

### 2. åˆå§‹åŒ–å…ƒæ•°æ®åº“

```bash
schematool -dbType mysql -initSchema
```

### 3. å¯åŠ¨ Hive æœåŠ¡

```bash
nohup hive --service metastore > metastore.log 2>&1 &
nohup hive --service hiveserver2 > hiveserver2.log 2>&1 &
```

### 4. éªŒè¯ç«¯å£ç›‘å¬

```bash
netstat -tnlp | grep 9083   # Metastore
netstat -tnlp | grep 10000  # HiveServer2
```

### 5. è¿æ¥ Hive

```bash
beeline -u jdbc:hive2://192.168.118.128:10000 -n hiveuser
```

---

## å…«ã€å®‰è£… Spark

### å¯åŠ¨ Spark æœåŠ¡

```bash
cd /usr/local/Spark/spark-3.5.6-bin-hadoop3
./sbin/start-all.sh
```

### å¯åŠ¨ Spark Shell

```bash
spark-shell --master local                # å•æœºæ¨¡å¼
spark-shell --master spark://IP:7077      # Standalone æ¨¡å¼
spark-shell --master yarn                 # YARN æ¨¡å¼ï¼ˆéœ€ Hadoop å·²å¯åŠ¨ï¼‰
```

### å¯åŠ¨ spark-sql

```bash
cd $SPARK_HOME
./bin/spark-sql
```

---

## ä¹ã€å¯åŠ¨ IntelliJ IDEA å¹¶é…ç½® Hadoop é¡¹ç›®

```bash
cd /home/cyh/ä¸‹è½½/idea-IU-251.26094.121/bin
./idea.sh
```

> ğŸ’¡ å»ºè®®ä½¿ç”¨ IDEA Ultimate ç‰ˆæœ¬ï¼Œä¾¿äºæ’ä»¶ç®¡ç†å’Œå¤§æ•°æ®å¼€å‘æ”¯æŒã€‚

---

## åã€å¸¸ç”¨å‘½ä»¤å‚è€ƒ

```bash
# åˆ é™¤æ–‡ä»¶å¤¹
rm -r ç›®å½•å             # é€ä¸ªç¡®è®¤åˆ é™¤
rm -rf ç›®å½•å            # å¼ºåˆ¶åˆ é™¤
rm -f æ–‡ä»¶å             # åˆ é™¤å•ä¸ªæ–‡ä»¶

# åˆ›å»ºç›®å½•
mkdir ç›®å½•å

# æŸ¥çœ‹å½“å‰è·¯å¾„
pwd
```

---

## å‚è€ƒé“¾æ¥

- [Mac VMware å®‰è£… CentOS 7](https://blog.csdn.net/vbirdbest/article/details/107375067)
- [M1 VMware Fusion å®‰è£… CentOS è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1XW4y1y7zv)
- [åœ¨ IntelliJ IDEA ä¸­ä½¿ç”¨ Hadoop æ•™ç¨‹](https://blog.csdn.net/shenpkun/article/details/130295946)
- [Ideaå®‰è£…è¿‡ç¨‹å›¾è§£](https://blog.csdn.net/hang_sugar/article/details/127331445)


## äºŒã€é¡¹ç›®æ‰§è¡Œä¸å¼€å‘æ€»ç»“

# ç¯å¢ƒæ­å»º

> ç‰ˆæœ¬éœ€æŒ‰ç…§å¦‚ä¸‹æŒ‰ç…§å¯¹ç…§ç‰ˆæœ¬ï¼Œè‹¥æŒ‰ç…§å…¶ä»–ç‰ˆæœ¬å¯èƒ½ä¼šå‡ºç°è½¯ä»¶ä¸å…¼å®¹æ— æ³•å®ç°è½¯ä»¶ä¹‹é—´çš„è°ƒç”¨

Hadoop3.2.0

Hive4.0

Spark3.3.3
JDK:java-1.8.0-openjdk-develï¼ˆarrch64ç‰ˆæœ¬ï¼‰



## å¸¸ç”¨å‘½ä»¤

```
#ç§»åŠ¨æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
mv [é€‰é¡¹] æºæ–‡ä»¶æˆ–ç›®å½• ç›®æ ‡ç›®å½• 
#è§£å‹æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
sudo tar -zxvf hadoop-3.2.0.tar.gz -C /usr/local/ 
#ç¼–è¾‘ç¯å¢ƒå˜é‡
vi ~/.bashrc 
#ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
source ~/.bashrc 
#æ–°å»ºæ–‡ä»¶å¤¹
mkdir æ–‡ä»¶å¤¹å 
#åˆ›å»ºå¤šçº§ç›®å½•ï¼ˆä¸€æ¬¡æ€§åˆ›å»ºçˆ¶å­ç›®å½•ï¼‰
mkdir -p /opt/hadoop/dfs/namenode 
#åˆ é™¤æ–‡ä»¶å¤¹
rmdir æ–‡ä»¶å¤¹å
-r æˆ– -Rï¼šé€’å½’åˆ é™¤ç›®å½•åŠå…¶å†…å®¹
-fï¼šå¼ºåˆ¶åˆ é™¤ï¼ˆä¸æç¤ºç¡®è®¤ï¼‰
-vï¼šæ˜¾ç¤ºåˆ é™¤è¿‡ç¨‹
#åˆ é™¤æ–‡ä»¶ä¸è¿›è¡Œç¡®è®¤
rm -rf ç›®å½•å/*
#è·å–å½“å‰æ‰€åœ¨ç›®å½•
pwd

```



## Hadoop3å•æœºä¼ªåˆ†å¸ƒé›†ç¾¤

### é…ç½®SSHå…å¯†é’¥ç™»å½•(Root)

åœ¨æœ¬æœºä¸Šä¿®æ”¹ `/etc/ssh/sshd_config`

```
vi /etc/ssh/sshd_config

PermitRootLogin yes          # å…è®¸rootç™»å½•
PubkeyAuthentication yes     # å¯ç”¨å…¬é’¥è®¤è¯
PasswordAuthentication yes   # å…è®¸å¯†ç è®¤è¯ï¼ˆä¸´æ—¶å¯ç”¨ï¼‰

systemctl restart sshd
```



```shell
#å…ˆå®‰è£…JDK
ssh-keygen -t rsa #ç”Ÿæˆå¯†é’¥ï¼ˆæ ¹æ®æç¤ºå¯ä»¥ä¸ç”¨è¾“å…¥ä»»ä½•å†…å®¹ï¼Œè¿ç»­æŒ‰4æ¬¡Enteré”®ç¡®è®¤å³å¯ï¼‰
å…å¯†ç™»é™†é…ç½®æˆåŠŸåç¬¬ä¸€æ¬¡ç™»é™†å¯†ç ä¸ºrootç”¨æˆ·å¯†ç 
```

![1](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/é¡¹ç›®æµç¨‹æ€»ç»“/img/1713912867089.png)

ï¼ˆ2ï¼‰ç”Ÿæˆå¯†é’¥æ“ä½œé»˜è®¤ä¼šåœ¨rootç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ªåŒ…å«æœ‰å¯†é’¥æ–‡ä»¶çš„.sshéšè—ç›®å½•ã€‚æ‰§è¡Œcd /root/.sshå‘½ä»¤è¿›å…¥.sshéšè—ç›®å½•ï¼Œåœ¨è¯¥ç›®å½•ä¸‹æ‰§è¡Œll -aå‘½ä»¤æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚å…¶ä¸­ï¼Œid_rsaå’Œid_rsa.pubè¿™ä¸¤ä¸ªæ–‡ä»¶åˆ†åˆ«æ˜¯ç§é’¥æ–‡ä»¶å’Œå…¬é’¥æ–‡ä»¶

ï¼ˆ3ï¼‰ä¸ºäº†ä¾¿äºæ–‡ä»¶é…ç½®å’Œè™šæ‹Ÿæœºé€šä¿¡ï¼Œé€šå¸¸ä¼šå¯¹ä¸»æœºåå’ŒIPåšæ˜ å°„é…ç½®ï¼Œåœ¨è™šæ‹Ÿæœºæ‰§è¡Œvi /etc/hostså‘½ä»¤ç¼–è¾‘æ˜ å°„æ–‡ä»¶hostsï¼Œåœ¨æ˜ å°„æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

è™šæ‹ŸæœºIP ä¸»æœºå

![2](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/é¡¹ç›®æµç¨‹æ€»ç»“/img/1713913456511.png)

ï¼ˆ4ï¼‰åœ¨è™šæ‹Ÿæœºä¸Šæ‰§è¡Œ"ssh-copy-id ä¸»æœºå"å‘½ä»¤ï¼Œå°†å…¬é’¥å¤åˆ¶åˆ°ç›¸å…³è”çš„è™šæ‹Ÿæœºï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰

![](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/é¡¹ç›®æµç¨‹æ€»ç»“/img/1713912867089.png)

ï¼ˆ5ï¼‰åœ¨æœ¬è™šæ‹Ÿæœºä¸Šæ‰§è¡Œ"ssh ç›®æ ‡æœºå™¨"å‘½ä»¤è¿æ¥åˆ°ç›®æ ‡æœºå™¨ä¸Šï¼Œè¿›è¡ŒéªŒè¯å…å¯†é’¥ç™»å½•æ“ä½œï¼Œæ­¤æ—¶æ— éœ€è¾“å…¥å¯†ç ä¾¿å¯ç›´æ¥ç™»å½•ç›®æ ‡è™šæ‹Ÿæœºè¿›è¡Œæ“ä½œï¼Œå¦‚éœ€è¿”å›æœ¬è™šæ‹Ÿæœºï¼Œæ‰§è¡Œexitå‘½ä»¤å³å¯ã€‚

å…³é—­é˜²ç«å¢™

æŸ¥çœ‹é˜²ç«å¢™çŠ¶æ€ï¼š

```sql
firewall-cmd --state
```

  åœæ­¢é˜²ç«å¢™æœåŠ¡ï¼š 

```
systemctl stop firewalld
```

  ç¦ç”¨é˜²ç«å¢™æœåŠ¡ï¼Œç¡®ä¿å…¶åœ¨ç³»ç»Ÿé‡æ–°å¯åŠ¨åä¸ä¼šè‡ªåŠ¨å¯åŠ¨

```
systemctl disable firewalld
```

Hadoop ä¼ªåˆ†å¸ƒé›†ç¾¤å®‰è£…

ä¸‹é¢å¼€å§‹åœ¨ 1 å° linux è™šæ‹Ÿæœºä¸Šå¼€å§‹å®‰è£… Hadoop3 ä¼ªåˆ†å¸ƒç¯å¢ƒï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ hadoop3.2.0 ç‰ˆæœ¬ï¼šhadoop-3.2.0.tar.gz

```
# ä¸‹è½½ hadoop-3.2.0.tar.gz
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz #å®˜ç½‘

wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz #å›½å†…é•œåƒä¸‹è½½

# ä½¿ç”¨ root æƒé™è§£å‹ï¼ˆéœ€ sudoï¼‰
sudo tar -zxvf hadoop-3.2.0.tar.gz -C /usr/local/ï¼ˆè‡ªå·±æŒ‡å®šçš„ç›®å½•ï¼‰
```



ï¼ˆ1ï¼‰æŠŠ hadoop-3.2.0.tar.gz å®‰è£…åŒ…ä¸Šä¼ åˆ° linux æœºå™¨çš„/usr/export ç›®å½•ä¸‹

ï¼ˆ2ï¼‰è§£å‹ hadoop å®‰è£…åŒ…åˆ°/usr/localç›®å½•ä¸‹

ï¼ˆ3ï¼‰è¿›å…¥é…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½• cd hadoop-3.2.0/etc/hadoop/

ï¼ˆ4ï¼‰ä¿®æ”¹ hadoop-env.sh æ–‡ä»¶ï¼Œå¢åŠ ç¯å¢ƒå˜é‡ä¿¡æ¯

```
export JAVA_HOME=/usr/local/java/jdk8u362-b09
export HADOOP_LOG_DIR=è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„
```

å…ˆåˆ›å»ºæ—¥å¿—ç›®å½•å†æ·»åŠ 

ï¼ˆ5ï¼‰ä¿®æ”¹ core-site.xml æ–‡ä»¶ï¼Œæ³¨æ„ fs.defaultFS å±æ€§ä¸­çš„ä¸»æœºåéœ€è¦å’Œä½ é…ç½®çš„ä¸»æœºåä¿æŒä¸€è‡´

```
<configuration>
   <property>
       <name>fs.defaultFS</name>
       <value>hdfs://my2308-host:9000</value>
   </property>
   <property>
       <name>hadoop.tmp.dir</name>
       <value>/export/data/hadoop_repo</value>
   </property>
</configuration>
```

ï¼ˆ6ï¼‰ä¿®æ”¹ hdfs-site.xml æ–‡ä»¶ï¼ŒæŠŠ hdfs ä¸­æ–‡ä»¶å‰¯æœ¬çš„æ•°é‡è®¾ç½®ä¸º 1ï¼Œå› ä¸ºç°åœ¨ä¼ªåˆ†å¸ƒé›†ç¾¤åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹

```
<configuration>
    <property>
       <name>dfs.replication</name>
       <value>1</value>
    </property>
</configuration>
```

ï¼ˆ7ï¼‰ä¿®æ”¹ mapred-site.xmlï¼Œè®¾ç½® mapreduce ä½¿ç”¨çš„èµ„æºè°ƒåº¦æ¡†æ¶

```
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
    </property>
</configuration>
```

ï¼ˆ8ï¼‰ä¿®æ”¹ yarn-site.xmlï¼Œè®¾ç½® yarn ä¸Šæ”¯æŒè¿è¡Œçš„æœåŠ¡å’Œç¯å¢ƒå˜é‡ç™½åå•

```
<configuration>

<!-- Site specific YARN configuration properties -->
   <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
   </property>
   <property>
       <name>yarn.nodemanager.env-whitelist</name>
       <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
   </property>
</configuration>
```

ï¼ˆ9ï¼‰æ ¼å¼åŒ– namenode

```
cd /export/servers/hadoop-3.2.0

bin/hdfs namenode -format
```

å¦‚æœåœ¨åé¢çš„æ—¥å¿—ä¿¡æ¯ä¸­èƒ½çœ‹åˆ°è¿™ä¸€è¡Œï¼Œåˆ™è¯´æ˜ namenode æ ¼å¼åŒ–æˆåŠŸã€‚
common.Storage: Storage directory xxx has been successfully
formatted.

(10ï¼‰ä¿®æ”¹start-dfs.shæ–‡ä»¶ï¼ˆåœ¨hadoop-3.2.0/sbinç›®å½•ä¸‹ï¼‰ï¼Œåœ¨æ–‡ä»¶å‰é¢å¢åŠ å¦‚ä¸‹å†…å®¹

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

ï¼ˆ11ï¼‰ä¿®æ”¹stop-dfs.shæ–‡ä»¶ï¼ˆåœ¨hadoop-3.2.0/sbinç›®å½•ä¸‹ï¼‰ï¼Œåœ¨æ–‡ä»¶å‰é¢å¢åŠ å¦‚ä¸‹å†…å®¹

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

ï¼ˆ12ï¼‰ä¿®æ”¹ start-yarn.shæ–‡ä»¶ï¼ˆåœ¨hadoop-3.2.0/sbinç›®å½•ä¸‹ï¼‰ï¼Œåœ¨æ–‡ä»¶å‰é¢å¢åŠ å¦‚ä¸‹å†…å®¹

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

ï¼ˆ13ï¼‰ä¿®æ”¹stop-yarn.sh æ–‡ä»¶ï¼ˆåœ¨hadoop-3.2.0/sbinç›®å½•ä¸‹ï¼‰ï¼Œåœ¨æ–‡ä»¶å‰é¢å¢åŠ å¦‚ä¸‹å†…å®¹

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

ï¼ˆ14ï¼‰å¯åŠ¨ hadoop é›†ç¾¤

```
sbin/start-all.sh
```

ï¼ˆ15ï¼‰éªŒè¯é›†ç¾¤è¿›ç¨‹ä¿¡æ¯

æ‰§è¡Œ jps å‘½ä»¤å¯ä»¥æŸ¥çœ‹é›†ç¾¤çš„è¿›ç¨‹ä¿¡æ¯ï¼Œé™¤äº†jps è¿™ä¸ªè¿›ç¨‹ä¹‹å¤–è¿˜éœ€è¦æœ‰ 5 ä¸ªè¿›ç¨‹æ‰è¯´æ˜é›†ç¾¤æ˜¯æ­£å¸¸å¯åŠ¨çš„

![4](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/é¡¹ç›®æµç¨‹æ€»ç»“/img/1713916541592.png)

è¿˜å¯ä»¥é€šè¿‡ webui ç•Œé¢æ¥éªŒè¯é›†ç¾¤æœåŠ¡æ˜¯å¦æ­£å¸¸:

hdfs webui ç•Œé¢ï¼šhttp://è™šæ‹ŸæœºIP:9870
yarn webui ç•Œé¢ï¼šhttp://è™šæ‹ŸæœºIP:8088

ï¼ˆ16ï¼‰åœæ­¢hadoopé›†ç¾¤

```
sbin/stop-all.sh
```

é…ç½®hadoopç¯å¢ƒå˜é‡

```
vi ~/.bashrc

export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source ~/.bashrc

hadoop version #éªŒè¯å®‰è£…
æ­£ç¡®è¾“å‡º
Hadoop 3.2.0
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git...
```

### Error

1.SSH æ— æ³•æ­£å¸¸è¿æ¥

```
ERROR: kex_exchange_identification: Connection closed by remote host
ERROR: Connection closed by 192.168.121.131 port 22
```

ipåœ°å€æœªè®¾ç½®æˆè‡ªå·±ä¸»æœºå¯¹åº”çš„åœ°å€

```
hostname -I    # æ˜¾ç¤ºæ‰€æœ‰ç½‘ç»œæ¥å£çš„ IP
# æˆ–
ip addr        # æŸ¥çœ‹è¯¦ç»†ç½‘ç»œé…ç½®ï¼ˆæ‰¾ inet å­—æ®µï¼‰
å°†ï¼ˆ3ï¼‰çš„ipåœ°å€ä¿®æ”¹ä¸ºæ˜¾ç¤ºçš„ipåœ°å€
ip my2308-host
```

2.å¦‚æœä¸Šè¿°SSHæ“ä½œæœªé…ç½®æˆåŠŸ,å°è¯•ä»¥ä¸‹æ“ä½œ

```
# 1. ç”Ÿæˆ SSH å¯†é’¥ï¼ˆå¦‚æœå°šæœªç”Ÿæˆï¼‰
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# 2. å°†å…¬é’¥æ·»åŠ åˆ°æˆæƒåˆ—è¡¨
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# 3. ä¿®æ”¹æƒé™ï¼ˆå…³é”®ï¼ï¼‰
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh

# 4. æµ‹è¯•å…å¯†ç™»å½•
ssh localhost
```

## Hive

### å®‰è£…MySQL

> å…ˆå®‰è£…wgetå‘½ä»¤

(1)**æ‰‹åŠ¨ä¸‹è½½RPMåŒ…**

```
wget http://repo.mysql.com/yum/mysql-8.0-community/el/9/aarch64/mysql-community-server-8.0.42-1.el9.aarch64.rpm
```

(2)**æ‰‹åŠ¨å®‰è£…**

```
yum localinstall mysql-community-server-8.0.42-1.el9.aarch64.rpm
```

(3)å®‰è£…MySQL

```
yum install mysql-server
```

ï¼ˆ4ï¼‰å®‰è£…MySQLå®¢æˆ·ç«¯å·¥å…·

```shell
sudo yum install mysql-community-client
```

(5)å¯åŠ¨MySQLæœåŠ¡

```
systemctl start mysql
```

(6)ä½¿ç”¨MySQL

```
mysql -uroot -p
exit; #é€€å‡º
systemctl status mysqld #æ£€æŸ¥æœåŠ¡çŠ¶æ€
```

>ç¬¬ä¸€æ¬¡ç™»é™†ä½¿ç”¨ä¸´æ—¶å¯†ç ï¼Œåœ¨æ—¥å¿—æŸ¥çœ‹
>
>sudo grep 'temporary password' /var/log/mysqld.log è·å–ä¸´æ—¶å¯†ç 

(7)ä¸‹è½½mysqlé©±åŠ¨

```
æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.33.tar.gz
# è§£å‹è·å– JAR æ–‡ä»¶
tar -xzvf mysql-connector-java-8.0.33.tar.gz
cp mysql-connector-java-8.0.33/mysql-connector-java-8.0.33.jar /ç›®æ ‡è·¯å¾„/
```



**é‡ç½® root å¯†ç ï¼ˆéœ€é‡å¯ MySQL**ï¼‰(å¦‚æœä¸´æ—¶å¯†ç è¿˜æ— æ³•ä½¿ç”¨)

```
#ä½¿ç”¨ mysql ç”¨æˆ·èº«ä»½å¯åŠ¨
sudo systemctl stop mysqld
sudo -u mysql mysqld --skip-grant-tables --skip-networking &

mysql -u root
#é‡ç½®å¯†ç 
FLUSH PRIVILEGES;
ALTER USER 'root'@'localhost' IDENTIFIED BY 'æ–°å¯†ç ';

#é‡å¯ MySQL æœåŠ¡
sudo pkill mysqld
sudo systemctl start mysqld
```

### Hiveå®‰è£…

ï¼ˆ1ï¼‰ä¸‹è½½apache-hive-3.1.3-bin.tar.gz(è¿™é‡Œç½‘å€å¯èƒ½å¤±æ•ˆï¼Œå»å®˜ç½‘ä¸‹è½½æœ€æ–°4.0ç‰ˆæœ¬å³å¯)

```
wget https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz.sha512
```

ï¼ˆ2ï¼‰è§£å‹apache-hive-3.1.3-bin.tar.gzåˆ°æŒ‡å®šç›®å½•ï¼Œä¾‹å¦‚ï¼š

```
tar -xzvf apache-hive-3.1.3-bin.tar.gz -C /export/servers
```

(3) é…ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨å…¨å±€é…ç½®æ–‡ä»¶/etc/profileï¼‰

```
export HIVE_HOME=//usr/local/Hive/apache-hive-4.0.1-bin
export HIVE_CONF_DIR=/usr/local/Hive/apache-hive-4.0.1-bin/conf
export PATH=$PATH:$HIVE_HOME/bin
```

source /etc/profileä½¿é…ç½®ç”Ÿæ•ˆ

(4)åœ¨hiveæ ¹ç›®å½•ä¸‹çš„confç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªhive-site.xmlæ–‡ä»¶ï¼Œå¹¶æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼š

```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<configuration>
    <!-- æ•°æ®åº“ start -->
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/hive_meta?useSSL=false</value>
      <description>mysqlè¿æ¥</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>mysqlé©±åŠ¨</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>æ•°æ®åº“ä½¿ç”¨ç”¨æˆ·å</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>123456</value>
      <description>æ•°æ®åº“å¯†ç </description>
    </property>
    <!-- æ•°æ®åº“ end -->

    <property> 
      <name>hive.metastore.warehouse.dir</name>
      <value>/hive/warehouse</value>
      <description>hiveä½¿ç”¨çš„HDFSç›®å½•</description>
    </property>

    <property> 
      <name>hive.cli.print.current.db</name>
      <value>true</value>
    </property>
    <property>
      <name>hive.support.concurrency</name>
      <value>true</value>
      <description>å¼€å¯Hiveçš„å¹¶å‘æ¨¡å¼</description>
    </property>
    <property>
      <name>hive.txn.manager</name>
      <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
      <description>ç”¨äºå¹¶å‘æ§åˆ¶çš„é”ç®¡ç†å™¨ç±»</description>
    </property>
    <property>
      <name>hive.server2.thrift.bind.host</name>
      <value>my2308-host</value>
      <description>hiveå¼€å¯çš„thriftServeråœ°å€</description>
    </property>

    <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
      <description>hiveå¼€å¯çš„thriftServerç«¯å£</description>
    </property>

    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>
    <!-- å…¶å®ƒ end -->
</configuration>
ç¤ºä¾‹é…ç½®ï¼ˆæœ‰äº›éœ€è¦ä¿®æ”¹ï¼‰

å®é™…é…ç½®
configuration>
    <!-- æ•°æ®åº“ start -->
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/hive_meta?createDatabaseIfNotExist=true&amp;serverTimezone=UTC&amp;useSSL=false</value>
      <description>mysqlè¿æ¥</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.cj.jdbc.Driver</value>
      <description>mysqlé©±åŠ¨</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
			value>hiveuser</value>
      <description>æ•°æ®åº“ä½¿ç”¨ç”¨æˆ·å</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>Hive123@!</value>
      <description>æ•°æ®åº“å¯†ç </description>
    </property>
    <!-- æ•°æ®åº“ end -->

<property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
      <description>hiveä½¿ç”¨çš„HDFSç›®å½•</description>
    </property>

    <property>
      <name>hive.cli.print.current.db</name>
      <value>true</value>
    </property>
    <property>
      <name>hive.support.concurrency</name>
      <value>true</value>
      <description>å¼€å¯Hiveçš„å¹¶å‘æ¨¡å¼</description>
    </property>
    <property>
      <name>hive.txn.manager</name>
      <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
      <description>ç”¨äºå¹¶å‘æ§åˆ¶çš„é”ç®¡ç†å™¨ç±»</description>
    </property>
    <property>
      <name>hive.server2.thrift.bind.host</name>
		value>my2308-host</value>
      <description>hiveå¼€å¯çš„thriftServeråœ°å€</description>
    </property>

    <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
      <description>hiveå¼€å¯çš„thriftServerç«¯å£</description>
    </property>

    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>
    <!-- å…¶å®ƒ end -->
</configuration>
```

(5)ä¿®æ”¹$HADOOP_HOME/etc/hadoop/core-site.xml å¼€å¯hadoopä»£ç†åŠŸèƒ½

```
<property>
       <name>hadoop.proxyuser.root.groups</name>
       <value>*</value>
       <description>é…ç½®è¶…çº§ç”¨æˆ·å…è®¸é€šè¿‡ä»£ç†ç”¨æˆ·æ‰€å±ç»„</description>
   </property>
   <property>
       <name>hadoop.proxyuser.root.hosts</name>
       <value>*</value>
       <description>é…ç½®è¶…çº§ç”¨æˆ·å…è®¸é€šè¿‡ä»£ç†è®¿é—®çš„ä¸»æœºèŠ‚ç‚¹</description>
   </property>
   <property>
       <name>hadoop.proxyuser.root.users</name>
       <value>*</value>
   </property>
```

(6)æ‹·è´hive-env.sh.templateæ¨¡ç‰ˆé…ç½®æ–‡ä»¶ä¸ºhive-env.sh

```
cp hive-env.sh.template hive-env.sh
```

(7)åœ¨hive-env.shæ–‡ä»¶ä¸­æ·»åŠ Hadoopç›®å½•ä½ç½®

HADOOP_HOME=/usr/local/hadoop3.2/hadoop-3.2.0

(8)å¯¹æ—¥å¿—æ–‡ä»¶æ”¹å

```
mv hive-log4j2.properties.template hive-log4j2.properties
```

(9)åœ¨MySQLä¸­åˆ›å»ºhiveç”¨çš„å…ƒæ•°æ®åº“hive_meta

```
create database hive_meta default charset utf8 collate utf8_general_ci;
```

(10)æ‹·è´mysqlé©±åŠ¨jar åˆ°/export/servers/apache-hive-3.1.3-bin/lib

```
cp mysql-connector-java-8.0.33/mysql-connector-java-8.0.33.jar /usr/local/Hive/apache-hive-4.0.1-bin/lib
```

(11)åˆ é™¤å†²çªçš„log4jï¼ˆlog4j-slf4j-impl-2.4.1.jarï¼‰

```
rm -f /usr/local/Hive/apache-hive-4.0.1-bin/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.4.1.jar
```

(12)hiveåˆå§‹åŒ–mysql

```
schematool -dbType mysql -initSchema
```

schematool æ˜¯ hive è‡ªå¸¦çš„ç®¡ç† schema çš„ç›¸å…³å·¥å…·ï¼Œåœ¨hiveæ ¹ç›®å½•ä¸­çš„binç›®å½•é‡Œã€‚

éšåï¼Œå¯ä»¥çœ‹åˆ°hive_metaæ•°æ®åº“ä¸­æœ‰å¾ˆå¤šè¡¨äº†

![](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/é¡¹ç›®æµç¨‹æ€»ç»“/img/1717937416614.png)

### å¯åŠ¨Hive

```
ä»¥å‘½ä»¤è¡Œæ–¹å¼å¯åŠ¨ï¼ˆåœ¨$HIVE_HOME/binç›®å½•ä¸‹ï¼‰
hive
ä»¥JDBCè¿æ¥å¯åŠ¨ï¼ˆbeelineæ–¹å¼è¿æ¥ï¼‰
Hiveå®‰è£…ç›®å½•ï¼š/usr/local/Hive/apache-hive-4.0.1-bin 
  nohup hive --service metastore \ > metastore.log 2>&1 & #åå°å¯åŠ¨ 
  netstat -tnlp | grep 9083 # åº”çœ‹åˆ°: LISTEN 0.0.0.0:9083/java 
  nohup hive --service hiveserver2 > hiveserver2.log 2>&1 & 
  #å¯åŠ¨Hiveserver2ï¼ˆç­‰å¾…10så·¦å³ï¼‰ <br>
  netstat -tnlp | grep 10000  # åº”çœ‹åˆ°: LISTEN 0.0.0.0:10000/java <br>
  beeline -u jdbc:hive2://192.168.118.128:10000 -n hiveuser #ä½¿Beeline 
```

### Error

1.æ—¥å¿—ç»‘å®šå†²çª

```
Hive Session ID = 242ef03b-5dca-4149-b904-de351d236969
Hive Session ID = c15dcafd-3f45-467f-889f-693c31650d98
Hive Session ID = 8a78a138-5228-4b33-a684-7ecb2e0bd2c6
Hive Session ID = b5d38c85-43ff-41ff-bc1d-e06ee278a497
.......
ä¸€ç›´å¡åœ¨è¯¥ç•Œé¢
```

```
# ç§»é™¤ Hive è‡ªå¸¦çš„å†²çªæ—¥å¿—åŒ…ï¼ˆä¿ç•™ä¸€ä¸ªå³å¯ï¼‰
mv /usr/local/Hive/apache-hive-4.0.1-bin/lib/log4j-slf4j-impl-2.18.0.jar /tmp/
```



## Spark 

(1)å®‰è£…

```
wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz

wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz
```

(2)é…ç½®ç¯å¢ƒå˜é‡

```
export SPARK_HOME=/usr/local/Spark/spark
export PATH=$PATH:$SPARK_HOME/bin

source /etc/profile

#å¯åŠ¨Spark-å‘½ä»¤è¡Œ
cd $SPARK_HOME
./bin/spark-sql
#å¯åŠ¨Sprark-shell;
cd $SPARK_HOME
./bin/spark-shell

```

(3)å¯åŠ¨Spark

# å®ç°é¡¹ç›®åŠŸèƒ½

#### å¯åŠ¨Hadoopé›†ç¾¤ï¼ˆä»æœ¬åœ°ä¸Šä¼ æ–‡ä»¶è‡³HDFSï¼‰

```shell
#å¯åŠ¨ HDFS
$HADOOP_HOME/sbin/start-all.sh
#æ–°å»º HDFS ç›®å½•å­˜æ”¾æ•°æ®
hdfs dfs -mkdir -p /user/hive/warehouse/data
#ä¸Šä¼ æ•°æ®åˆ° HDFS
hdfs dfs -put /root/monitor_flow_action.txt /user/hive/warehouse/data/
hdfs dfs -put /root/monitor_camera_info.txt /user/hive/warehouse/data/
#ç¡®è®¤æ•°æ®ä¸Šä¼ æˆåŠŸ
hdfs dfs -ls /user/hive/warehouse/data/
jps #æ£€æŸ¥æ˜¯å¦å¯åŠ¨æˆåŠŸ
```

#### å¯åŠ¨Hive(ç”¨æ¥åˆ›å»ºåœ¨åˆ›å»ºæ•°æ®åº“ä»¥åŠè¡¨)

```shell
nohup hive --service metastore \ \> metastore.log 2>&1 & #åå°å¯åŠ¨metastore

netstat -tnlp | grep 9083 #æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ

nohup hive --service hiveserver2 > hiveserver2.log 2>&1 & #å¯åŠ¨Hiveserver2ï¼ˆç­‰å¾…10så·¦å³ï¼‰

netstat -tnlp | grep 10000 

beeline -u jdbc:hive2://192.168.118.128:10000 -n hiveuser #ä½¿ç”¨ Beeline è¿æ¥å¹¶éªŒè¯
```

#### åˆ›å»ºæ•°æ®åº“åŠè¡¨(å·²ç»åˆ›å»ºè¿‡å¯ç›´æ¥ä½¿ç”¨)

```shell
USE traffic_db;
show tables;

CREATE DATABASE IF NOT EXISTS traffic_db; 
USE traffic_db;

CREATE TABLE IF NOT EXISTS monitor_flow_action ( #å¡å£æ‘„åƒè¡¨
  date STRING,
  monitor_id STRING,
  camera_id STRING,
  car STRING,
  action_time STRING,
  speed INT,
  road_id STRING,
  area_id STRING
)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS monitor_camera_info ( #è½¦è¾†æµåŠ¨è¡¨
  monitor_id STRING,
  camera_id STRING
)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

#å¯¼å…¥æ•°æ®
LOAD DATA INPATH '/user/hive/warehouse/data/monitor_flow_action.txt' INTO TABLE monitor_flow_action;
LOAD DATA INPATH '/user/hive/warehouse/data/monitor_camera_info.txt' INTO TABLE monitor_camera_info;
#æ£€æŸ¥æ˜¯å¦å¯¼å…¥
SELECT * FROM monitor_flow_action LIMIT 5;
SELECT * FROM monitor_camera_info LIMIT 5;
```

#### å¯åŠ¨Spark-sql

```
cd $SPARK_HOME
./bin/spark-sql

USE traffic_db;
```

é¡¹ç›®è¦æ±‚

```sql
1.ç»Ÿè®¡æ­£å¸¸å¡å£æ•°é‡ï¼š()
æç¤ºï¼šmonitor_flow_actionè¡¨ä¸­å¡å£å·å»é‡åçš„æ•°é‡å³ä¸ºæ­£å¸¸å¡å£æ•°é‡
		
SELECT COUNT(DISTINCT monitor_id) 
FROM monitor_flow_action;

COUNT() æ˜¯ä¸€ä¸ªèšåˆå‡½æ•°ï¼Œç”¨äºè®¡ç®—è¡Œæ•°
DISTINCT å…³é”®å­—è¡¨ç¤ºåªè®¡ç®—å”¯ä¸€ä¸é‡å¤çš„å€¼
monitor_id æ˜¯å¡å£ç¼–å·å­—æ®µ


2.ç»Ÿè®¡è½¦æµé‡æ’åå‰3çš„å¡å£å·ï¼ˆæç¿±å¥‡ï¼‰
SELECT monitor_id, COUNT(*)  AS traffic_volume
FROM monitor_flow_action
GROUP BY monitor_id
ORDER BY traffic_volume DESC
LIMIT 3;


3.æŸ¥è¯¢è½¦ç‰Œå·ä¸ºæ·±M37437çš„è½¦è¾†åœ¨2018å¹´4æœˆ26æ—¥è‡³2018å¹´4æœˆ27æ—¥æœŸé—´çš„è¿è¡Œè½¨è¿¹ï¼ˆæ—¶é—´å’Œå¡å£å·ï¼‰ï¼ˆå´”å®‡æ¶µï¼‰
SELECT action_time, monitor_id
FROM monitor_flow_action
WHERE car = 'æ·±M37437' 
AND date BETWEEN '2018-04-26' AND '2018-04-27'
ORDER BY action_time;




4.ç»Ÿè®¡è½¦è¾†é«˜é€Ÿé€šè¿‡çš„å¡å£Top5ï¼ˆç‹é¹ï¼Œæ¨æ»¡çªï¼‰
-- ç»Ÿè®¡è½¦è¾†é«˜é€Ÿé€šè¿‡çš„å¡å£Top5
-- ç»Ÿè®¡è½¦è¾†é«˜é€Ÿé€šè¿‡çš„å¡å£Top5
WITH speed_category AS (
  SELECT 
    monitor_id,
    CASE 
      WHEN speed > 120 THEN 'high'
      WHEN speed > 90 THEN 'medium_high'
      WHEN speed >= 60 THEN 'medium'
      ELSE 'low'
    END AS speed_level
  FROM monitor_flow_action
),
speed_stats AS (
  SELECT 
    monitor_id,
    COUNT(CASE WHEN speed_level = 'high' THEN 1 END) AS high_count,
    COUNT(CASE WHEN speed_level = 'medium_high' THEN 1 END) AS medium_high_count,
    COUNT(CASE WHEN speed_level = 'medium' THEN 1 END) AS medium_count,
    COUNT(CASE WHEN speed_level = 'low' THEN 1 END) AS low_count,
    COUNT(*) AS total_count
  FROM speed_category
  GROUP BY monitor_id
  HAVING COUNT(CASE WHEN speed_level = 'high' THEN 1 END) > 0
)
SELECT 
  monitor_id AS `å¡å£ç¼–å·`,
  high_count AS `é«˜é€Ÿè½¦è¾†æ•°`,
  medium_high_count AS `ä¸­é«˜é€Ÿè½¦è¾†æ•°`,
  medium_count AS `ä¸­é€Ÿè½¦è¾†æ•°`,
  low_count AS `ä½é€Ÿè½¦è¾†æ•°`,
  total_count AS `æ€»è½¦è¾†æ•°`,
  ROUND(high_count * 100.0 / total_count, 2) AS `é«˜é€Ÿå æ¯”(%)`
FROM speed_stats
ORDER BY 
  high_count DESC,
  medium_high_count DESC,
  medium_count DESC,
  low_count DESC
LIMIT 5;

5.ç»Ÿè®¡æ¯ä¸ªå¡å£é€šè¿‡é€Ÿåº¦æœ€å¿«çš„å‰3è¾†è½¦ï¼ˆç‹æ³°ç« ï¼‰
SELECT monitor_id, car, speed
FROM (
    SELECT 
        monitor_id,
        car,
        speed,
        ROW_NUMBER() OVER (PARTITION BY monitor_id ORDER BY speed DESC) as rank 
    FROM monitor_flow_action																					#å¼€çª—å‡½æ•°ï¼ŒæŒ‰å¡å£Idåˆ†ç»„ï¼Œæ¯ä¸ªå¡å£å•ç‹¬è®¡ç®—æ’å
) 
WHERE rank <= 3
ORDER BY monitor_id, rank;
```

