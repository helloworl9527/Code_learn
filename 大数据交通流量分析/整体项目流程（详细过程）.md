# 环境搭建

> 版本需按照如下按照对照版本，若按照其他版本可能会出现软件不兼容无法实现软件之间的调用

Hadoop3.2.0

Hive4.0

Spark3.3.3
JDK:java-1.8.0-openjdk-devel（arrch64版本）



## 常用命令

```
#移动文件到指定目录
mv [选项] 源文件或目录 目标目录 
#解压文件到指定目录
sudo tar -zxvf hadoop-3.2.0.tar.gz -C /usr/local/ 
#编辑环境变量
vi ~/.bashrc 
#使环境变量生效
source ~/.bashrc 
#新建文件夹
mkdir 文件夹名 
#创建多级目录（一次性创建父子目录）
mkdir -p /opt/hadoop/dfs/namenode 
#删除文件夹
rmdir 文件夹名
-r 或 -R：递归删除目录及其内容
-f：强制删除（不提示确认）
-v：显示删除过程
#删除文件不进行确认
rm -rf 目录名/*
#获取当前所在目录
pwd

```



## Hadoop3单机伪分布集群

### 配置SSH免密钥登录(Root)

在本机上修改 `/etc/ssh/sshd_config`

```
vi /etc/ssh/sshd_config

PermitRootLogin yes          # 允许root登录
PubkeyAuthentication yes     # 启用公钥认证
PasswordAuthentication yes   # 允许密码认证（临时启用）

systemctl restart sshd
```



```shell
#先安装JDK
ssh-keygen -t rsa #生成密钥（根据提示可以不用输入任何内容，连续按4次Enter键确认即可）
免密登陆配置成功后第一次登陆密码为root用户密码
```

![1](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/项目流程总结/img/1713912867089.png)

（2）生成密钥操作默认会在root目录下生成一个包含有密钥文件的.ssh隐藏目录。执行cd /root/.ssh命令进入.ssh隐藏目录，在该目录下执行ll -a命令查看当前目录下的所有文件。其中，id_rsa和id_rsa.pub这两个文件分别是私钥文件和公钥文件

（3）为了便于文件配置和虚拟机通信，通常会对主机名和IP做映射配置，在虚拟机执行vi /etc/hosts命令编辑映射文件hosts，在映射文件中添加如下内容：

虚拟机IP 主机名

![2](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/项目流程总结/img/1713913456511.png)

（4）在虚拟机上执行"ssh-copy-id 主机名"命令，将公钥复制到相关联的虚拟机（包括自身）

![](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/项目流程总结/img/1713912867089.png)

（5）在本虚拟机上执行"ssh 目标机器"命令连接到目标机器上，进行验证免密钥登录操作，此时无需输入密码便可直接登录目标虚拟机进行操作，如需返回本虚拟机，执行exit命令即可。

关闭防火墙

查看防火墙状态：

```sql
firewall-cmd --state
```

  停止防火墙服务： 

```
systemctl stop firewalld
```

  禁用防火墙服务，确保其在系统重新启动后不会自动启动

```
systemctl disable firewalld
```

Hadoop 伪分布集群安装

下面开始在 1 台 linux 虚拟机上开始安装 Hadoop3 伪分布环境，在这里我们使用 hadoop3.2.0 版本：hadoop-3.2.0.tar.gz

```
# 下载 hadoop-3.2.0.tar.gz
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz #官网

wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz #国内镜像下载

# 使用 root 权限解压（需 sudo）
sudo tar -zxvf hadoop-3.2.0.tar.gz -C /usr/local/（自己指定的目录）
```



（1）把 hadoop-3.2.0.tar.gz 安装包上传到 linux 机器的/usr/export 目录下

（2）解压 hadoop 安装包到/usr/local目录下

（3）进入配置文件所在目录 cd hadoop-3.2.0/etc/hadoop/

（4）修改 hadoop-env.sh 文件，增加环境变量信息

```
export JAVA_HOME=/usr/local/java/jdk8u362-b09
export HADOOP_LOG_DIR=自定义日志路径
```

先创建日志目录再添加

（5）修改 core-site.xml 文件，注意 fs.defaultFS 属性中的主机名需要和你配置的主机名保持一致

```
<configuration>
   <property>
       <name>fs.defaultFS</name>
       <value>hdfs://my2308-host:9000</value>
   </property>
   <property>
       <name>hadoop.tmp.dir</name>
       <value>/export/data/hadoop_repo</value>
   </property>
</configuration>
```

（6）修改 hdfs-site.xml 文件，把 hdfs 中文件副本的数量设置为 1，因为现在伪分布集群只有一个节点

```
<configuration>
    <property>
       <name>dfs.replication</name>
       <value>1</value>
    </property>
</configuration>
```

（7）修改 mapred-site.xml，设置 mapreduce 使用的资源调度框架

```
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
    </property>
</configuration>
```

（8）修改 yarn-site.xml，设置 yarn 上支持运行的服务和环境变量白名单

```
<configuration>

<!-- Site specific YARN configuration properties -->
   <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
   </property>
   <property>
       <name>yarn.nodemanager.env-whitelist</name>
       <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
   </property>
</configuration>
```

（9）格式化 namenode

```
cd /export/servers/hadoop-3.2.0

bin/hdfs namenode -format
```

如果在后面的日志信息中能看到这一行，则说明 namenode 格式化成功。
common.Storage: Storage directory xxx has been successfully
formatted.

(10）修改start-dfs.sh文件（在hadoop-3.2.0/sbin目录下），在文件前面增加如下内容

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

（11）修改stop-dfs.sh文件（在hadoop-3.2.0/sbin目录下），在文件前面增加如下内容

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

（12）修改 start-yarn.sh文件（在hadoop-3.2.0/sbin目录下），在文件前面增加如下内容

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

（13）修改stop-yarn.sh 文件（在hadoop-3.2.0/sbin目录下），在文件前面增加如下内容

```
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

（14）启动 hadoop 集群

```
sbin/start-all.sh
```

（15）验证集群进程信息

执行 jps 命令可以查看集群的进程信息，除了jps 这个进程之外还需要有 5 个进程才说明集群是正常启动的

![4](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/项目流程总结/img/1713916541592.png)

还可以通过 webui 界面来验证集群服务是否正常:

hdfs webui 界面：http://虚拟机IP:9870
yarn webui 界面：http://虚拟机IP:8088

（16）停止hadoop集群

```
sbin/stop-all.sh
```

配置hadoop环境变量

```
vi ~/.bashrc

export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source ~/.bashrc

hadoop version #验证安装
正确输出
Hadoop 3.2.0
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git...
```

### Error

1.SSH 无法正常连接

```
ERROR: kex_exchange_identification: Connection closed by remote host
ERROR: Connection closed by 192.168.121.131 port 22
```

ip地址未设置成自己主机对应的地址

```
hostname -I    # 显示所有网络接口的 IP
# 或
ip addr        # 查看详细网络配置（找 inet 字段）
将（3）的ip地址修改为显示的ip地址
ip my2308-host
```

2.如果上述SSH操作未配置成功,尝试以下操作

```
# 1. 生成 SSH 密钥（如果尚未生成）
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# 2. 将公钥添加到授权列表
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# 3. 修改权限（关键！）
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh

# 4. 测试免密登录
ssh localhost
```

## Hive

### 安装MySQL

> 先安装wget命令

(1)**手动下载RPM包**

```
wget http://repo.mysql.com/yum/mysql-8.0-community/el/9/aarch64/mysql-community-server-8.0.42-1.el9.aarch64.rpm
```

(2)**手动安装**

```
yum localinstall mysql-community-server-8.0.42-1.el9.aarch64.rpm
```

(3)安装MySQL

```
yum install mysql-server
```

（4）安装MySQL客户端工具

```shell
sudo yum install mysql-community-client
```

(5)启动MySQL服务

```
systemctl start mysql
```

(6)使用MySQL

```
mysql -uroot -p
exit; #退出
systemctl status mysqld #检查服务状态
```

>第一次登陆使用临时密码，在日志查看
>
>sudo grep 'temporary password' /var/log/mysqld.log 获取临时密码

(7)下载mysql驱动

```
手动下载并放置
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.33.tar.gz
# 解压获取 JAR 文件
tar -xzvf mysql-connector-java-8.0.33.tar.gz
cp mysql-connector-java-8.0.33/mysql-connector-java-8.0.33.jar /目标路径/
```



**重置 root 密码（需重启 MySQL**）(如果临时密码还无法使用)

```
#使用 mysql 用户身份启动
sudo systemctl stop mysqld
sudo -u mysql mysqld --skip-grant-tables --skip-networking &

mysql -u root
#重置密码
FLUSH PRIVILEGES;
ALTER USER 'root'@'localhost' IDENTIFIED BY '新密码';

#重启 MySQL 服务
sudo pkill mysqld
sudo systemctl start mysqld
```

### Hive安装

（1）下载apache-hive-3.1.3-bin.tar.gz(这里网址可能失效，去官网下载最新4.0版本即可)

```
wget https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz.sha512
```

（2）解压apache-hive-3.1.3-bin.tar.gz到指定目录，例如：

```
tar -xzvf apache-hive-3.1.3-bin.tar.gz -C /export/servers
```

(3) 配置环境变量（在全局配置文件/etc/profile）

```
export HIVE_HOME=//usr/local/Hive/apache-hive-4.0.1-bin
export HIVE_CONF_DIR=/usr/local/Hive/apache-hive-4.0.1-bin/conf
export PATH=$PATH:$HIVE_HOME/bin
```

source /etc/profile使配置生效

(4)在hive根目录下的conf目录下，创建一个hive-site.xml文件，并添加如下内容：

```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<configuration>
    <!-- 数据库 start -->
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/hive_meta?useSSL=false</value>
      <description>mysql连接</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>mysql驱动</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>数据库使用用户名</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>123456</value>
      <description>数据库密码</description>
    </property>
    <!-- 数据库 end -->

    <property> 
      <name>hive.metastore.warehouse.dir</name>
      <value>/hive/warehouse</value>
      <description>hive使用的HDFS目录</description>
    </property>

    <property> 
      <name>hive.cli.print.current.db</name>
      <value>true</value>
    </property>
    <property>
      <name>hive.support.concurrency</name>
      <value>true</value>
      <description>开启Hive的并发模式</description>
    </property>
    <property>
      <name>hive.txn.manager</name>
      <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
      <description>用于并发控制的锁管理器类</description>
    </property>
    <property>
      <name>hive.server2.thrift.bind.host</name>
      <value>my2308-host</value>
      <description>hive开启的thriftServer地址</description>
    </property>

    <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
      <description>hive开启的thriftServer端口</description>
    </property>

    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>
    <!-- 其它 end -->
</configuration>
示例配置（有些需要修改）

实际配置
configuration>
    <!-- 数据库 start -->
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://localhost:3306/hive_meta?createDatabaseIfNotExist=true&amp;serverTimezone=UTC&amp;useSSL=false</value>
      <description>mysql连接</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.cj.jdbc.Driver</value>
      <description>mysql驱动</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
			value>hiveuser</value>
      <description>数据库使用用户名</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>Hive123@!</value>
      <description>数据库密码</description>
    </property>
    <!-- 数据库 end -->

<property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
      <description>hive使用的HDFS目录</description>
    </property>

    <property>
      <name>hive.cli.print.current.db</name>
      <value>true</value>
    </property>
    <property>
      <name>hive.support.concurrency</name>
      <value>true</value>
      <description>开启Hive的并发模式</description>
    </property>
    <property>
      <name>hive.txn.manager</name>
      <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
      <description>用于并发控制的锁管理器类</description>
    </property>
    <property>
      <name>hive.server2.thrift.bind.host</name>
		value>my2308-host</value>
      <description>hive开启的thriftServer地址</description>
    </property>

    <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
      <description>hive开启的thriftServer端口</description>
    </property>

    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>
    <!-- 其它 end -->
</configuration>
```

(5)修改$HADOOP_HOME/etc/hadoop/core-site.xml 开启hadoop代理功能

```
<property>
       <name>hadoop.proxyuser.root.groups</name>
       <value>*</value>
       <description>配置超级用户允许通过代理用户所属组</description>
   </property>
   <property>
       <name>hadoop.proxyuser.root.hosts</name>
       <value>*</value>
       <description>配置超级用户允许通过代理访问的主机节点</description>
   </property>
   <property>
       <name>hadoop.proxyuser.root.users</name>
       <value>*</value>
   </property>
```

(6)拷贝hive-env.sh.template模版配置文件为hive-env.sh

```
cp hive-env.sh.template hive-env.sh
```

(7)在hive-env.sh文件中添加Hadoop目录位置

HADOOP_HOME=/usr/local/hadoop3.2/hadoop-3.2.0

(8)对日志文件改名

```
mv hive-log4j2.properties.template hive-log4j2.properties
```

(9)在MySQL中创建hive用的元数据库hive_meta

```
create database hive_meta default charset utf8 collate utf8_general_ci;
```

(10)拷贝mysql驱动jar 到/export/servers/apache-hive-3.1.3-bin/lib

```
cp mysql-connector-java-8.0.33/mysql-connector-java-8.0.33.jar /usr/local/Hive/apache-hive-4.0.1-bin/lib
```

(11)删除冲突的log4j（log4j-slf4j-impl-2.4.1.jar）

```
rm -f /usr/local/Hive/apache-hive-4.0.1-bin/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.4.1.jar
```

(12)hive初始化mysql

```
schematool -dbType mysql -initSchema
```

schematool 是 hive 自带的管理 schema 的相关工具，在hive根目录中的bin目录里。

随后，可以看到hive_meta数据库中有很多表了

![](/Users/apple/Library/Mobile Documents/com~apple~CloudDocs/Downloads/项目流程总结/img/1717937416614.png)

### 启动Hive

```
以命令行方式启动（在$HIVE_HOME/bin目录下）
hive
以JDBC连接启动（beeline方式连接）
Hive安装目录：/usr/local/Hive/apache-hive-4.0.1-bin 
  nohup hive --service metastore \ > metastore.log 2>&1 & #后台启动 
  netstat -tnlp | grep 9083 # 应看到: LISTEN 0.0.0.0:9083/java 
  nohup hive --service hiveserver2 > hiveserver2.log 2>&1 & 
  #启动Hiveserver2（等待10s左右） <br>
  netstat -tnlp | grep 10000  # 应看到: LISTEN 0.0.0.0:10000/java <br>
  beeline -u jdbc:hive2://192.168.118.128:10000 -n hiveuser #使Beeline 
```

### Error

1.日志绑定冲突

```
Hive Session ID = 242ef03b-5dca-4149-b904-de351d236969
Hive Session ID = c15dcafd-3f45-467f-889f-693c31650d98
Hive Session ID = 8a78a138-5228-4b33-a684-7ecb2e0bd2c6
Hive Session ID = b5d38c85-43ff-41ff-bc1d-e06ee278a497
.......
一直卡在该界面
```

```
# 移除 Hive 自带的冲突日志包（保留一个即可）
mv /usr/local/Hive/apache-hive-4.0.1-bin/lib/log4j-slf4j-impl-2.18.0.jar /tmp/
```



## Spark 

(1)安装

```
wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz

wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz
```

(2)配置环境变量

```
export SPARK_HOME=/usr/local/Spark/spark
export PATH=$PATH:$SPARK_HOME/bin

source /etc/profile

#启动Spark-命令行
cd $SPARK_HOME
./bin/spark-sql
#启动Sprark-shell;
cd $SPARK_HOME
./bin/spark-shell

```

(3)启动Spark

# 实现项目功能

#### 启动Hadoop集群（从本地上传文件至HDFS）

```shell
#启动 HDFS
$HADOOP_HOME/sbin/start-all.sh
#新建 HDFS 目录存放数据
hdfs dfs -mkdir -p /user/hive/warehouse/data
#上传数据到 HDFS
hdfs dfs -put /root/monitor_flow_action.txt /user/hive/warehouse/data/
hdfs dfs -put /root/monitor_camera_info.txt /user/hive/warehouse/data/
#确认数据上传成功
hdfs dfs -ls /user/hive/warehouse/data/
jps #检查是否启动成功
```

#### 启动Hive(用来创建在创建数据库以及表)

```shell
nohup hive --service metastore \ \> metastore.log 2>&1 & #后台启动metastore

netstat -tnlp | grep 9083 #查看是否启动成功

nohup hive --service hiveserver2 > hiveserver2.log 2>&1 & #启动Hiveserver2（等待10s左右）

netstat -tnlp | grep 10000 

beeline -u jdbc:hive2://192.168.118.128:10000 -n hiveuser #使用 Beeline 连接并验证
```

#### 创建数据库及表(已经创建过可直接使用)

```shell
USE traffic_db;
show tables;

CREATE DATABASE IF NOT EXISTS traffic_db; 
USE traffic_db;

CREATE TABLE IF NOT EXISTS monitor_flow_action ( #卡口摄像表
  date STRING,
  monitor_id STRING,
  camera_id STRING,
  car STRING,
  action_time STRING,
  speed INT,
  road_id STRING,
  area_id STRING
)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS monitor_camera_info ( #车辆流动表
  monitor_id STRING,
  camera_id STRING
)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

#导入数据
LOAD DATA INPATH '/user/hive/warehouse/data/monitor_flow_action.txt' INTO TABLE monitor_flow_action;
LOAD DATA INPATH '/user/hive/warehouse/data/monitor_camera_info.txt' INTO TABLE monitor_camera_info;
#检查是否导入
SELECT * FROM monitor_flow_action LIMIT 5;
SELECT * FROM monitor_camera_info LIMIT 5;
```

#### 启动Spark-sql

```
cd $SPARK_HOME
./bin/spark-sql

USE traffic_db;
```

项目要求

```sql
1.统计正常卡口数量：()
提示：monitor_flow_action表中卡口号去重后的数量即为正常卡口数量
		
SELECT COUNT(DISTINCT monitor_id) 
FROM monitor_flow_action;

COUNT() 是一个聚合函数，用于计算行数
DISTINCT 关键字表示只计算唯一不重复的值
monitor_id 是卡口编号字段


2.统计车流量排名前3的卡口号（李翱奇）
SELECT monitor_id, COUNT(*)  AS traffic_volume
FROM monitor_flow_action
GROUP BY monitor_id
ORDER BY traffic_volume DESC
LIMIT 3;


3.查询车牌号为深M37437的车辆在2018年4月26日至2018年4月27日期间的运行轨迹（时间和卡口号）（崔宇涵）
SELECT action_time, monitor_id
FROM monitor_flow_action
WHERE car = '深M37437' 
AND date BETWEEN '2018-04-26' AND '2018-04-27'
ORDER BY action_time;




4.统计车辆高速通过的卡口Top5（王鹏，杨满琪）
-- 统计车辆高速通过的卡口Top5
-- 统计车辆高速通过的卡口Top5
WITH speed_category AS (
  SELECT 
    monitor_id,
    CASE 
      WHEN speed > 120 THEN 'high'
      WHEN speed > 90 THEN 'medium_high'
      WHEN speed >= 60 THEN 'medium'
      ELSE 'low'
    END AS speed_level
  FROM monitor_flow_action
),
speed_stats AS (
  SELECT 
    monitor_id,
    COUNT(CASE WHEN speed_level = 'high' THEN 1 END) AS high_count,
    COUNT(CASE WHEN speed_level = 'medium_high' THEN 1 END) AS medium_high_count,
    COUNT(CASE WHEN speed_level = 'medium' THEN 1 END) AS medium_count,
    COUNT(CASE WHEN speed_level = 'low' THEN 1 END) AS low_count,
    COUNT(*) AS total_count
  FROM speed_category
  GROUP BY monitor_id
  HAVING COUNT(CASE WHEN speed_level = 'high' THEN 1 END) > 0
)
SELECT 
  monitor_id AS `卡口编号`,
  high_count AS `高速车辆数`,
  medium_high_count AS `中高速车辆数`,
  medium_count AS `中速车辆数`,
  low_count AS `低速车辆数`,
  total_count AS `总车辆数`,
  ROUND(high_count * 100.0 / total_count, 2) AS `高速占比(%)`
FROM speed_stats
ORDER BY 
  high_count DESC,
  medium_high_count DESC,
  medium_count DESC,
  low_count DESC
LIMIT 5;

5.统计每个卡口通过速度最快的前3辆车（王泰章）
SELECT monitor_id, car, speed
FROM (
    SELECT 
        monitor_id,
        car,
        speed,
        ROW_NUMBER() OVER (PARTITION BY monitor_id ORDER BY speed DESC) as rank 
    FROM monitor_flow_action																					#开窗函数，按卡口Id分组，每个卡口单独计算排名
) 
WHERE rank <= 3
ORDER BY monitor_id, rank;
```

